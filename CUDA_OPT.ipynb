{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install nvcc4jupyter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sE0soC8hBEd6",
        "outputId": "7ef8d196-d7af-443a-dc0d-7311f0f2d55f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvcc4jupyter\n",
            "  Downloading nvcc4jupyter-1.2.1-py3-none-any.whl.metadata (5.1 kB)\n",
            "Downloading nvcc4jupyter-1.2.1-py3-none-any.whl (10 kB)\n",
            "Installing collected packages: nvcc4jupyter\n",
            "Successfully installed nvcc4jupyter-1.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext nvcc4jupyter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1wrPhlbvBM6j",
        "outputId": "92086689-e816-4f1e-8ca6-242a2618386b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected platform \"Colab\". Running its setup...\n",
            "Source files will be saved in \"/tmp/tmpx0mtm9rw\".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile cuda_kernel.cu\n",
        "#include <cuda_runtime.h>\n",
        "#include <cmath>\n",
        "#include <iostream>\n",
        "#include <vector>\n",
        "#include <iomanip>\n",
        "#include <cstdlib>\n",
        "\n",
        "#define WARP_SIZE 32\n",
        "\n",
        "// math functions according to the problem statement\n",
        "__device__ __forceinline__ float fast_sin(float x) { return __sinf(x); }\n",
        "__device__ __forceinline__ float fast_cos(float x) { return __cosf(x); }\n",
        "__device__ __forceinline__ float fast_log(float x) { return __logf(x); }\n",
        "__device__ __forceinline__ float fast_exp(float x) { return __expf(x); }\n",
        "\n",
        "__global__ void transform_kernel(const float4* __restrict__ input, float4* __restrict__ output, float* __restrict__ global_sum, int n) {\n",
        "\n",
        "    // shared memory for both data and results\n",
        "    extern __shared__ float shared_data[];\n",
        "\n",
        "    const int tid = threadIdx.x;\n",
        "    const int lane = tid % WARP_SIZE;\n",
        "    const int warp_id = tid / WARP_SIZE;\n",
        "    const int block_size = blockDim.x;\n",
        "    const int block_start = blockIdx.x * block_size * 4;\n",
        "    const int warp_start = block_start + warp_id * 32;\n",
        "\n",
        "    // each thread loading one element for its warp\n",
        "    for (int i = lane; i < 32; i += WARP_SIZE) {\n",
        "        const int global_idx = warp_start + i;\n",
        "        if (global_idx < n) {\n",
        "            const int vec_idx = global_idx / 4;\n",
        "            const int elem_idx = global_idx % 4;\n",
        "            float4 data = input[vec_idx];\n",
        "\n",
        "            // extracting correct component from the float4\n",
        "            switch (elem_idx) {\n",
        "                case 0: shared_data[warp_id * 32 + i] = data.x; break;\n",
        "                case 1: shared_data[warp_id * 32 + i] = data.y; break;\n",
        "                case 2: shared_data[warp_id * 32 + i] = data.z; break;\n",
        "                case 3: shared_data[warp_id * 32 + i] = data.w; break;\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "    __syncthreads();\n",
        "\n",
        "    float warp_sin_sum = 0.0f;\n",
        "    const int local_idx = warp_id * 32 + lane;\n",
        "    const int global_idx = warp_start + lane;\n",
        "\n",
        "    if (global_idx < n) {\n",
        "        const int pos_in_32 = lane;\n",
        "        const int mod_4 = pos_in_32 % 4;\n",
        "        const float current_val = shared_data[local_idx];\n",
        "        float result = 0.0f;\n",
        "\n",
        "        // applying transformations based on position in 32 element block\n",
        "        if (pos_in_32 < 16) {\n",
        "            switch (mod_4) {\n",
        "                case 0: result = fast_sin(current_val); break;\n",
        "                case 1: result = fast_cos(current_val); break;\n",
        "                case 2: result = (current_val > 0.0f) ? fast_log(current_val) : -INFINITY; break;\n",
        "                case 3: result = fast_exp(current_val); break;\n",
        "            }\n",
        "        } else {\n",
        "            const int ref_pos = pos_in_32 - 16;\n",
        "            const float ref_val = shared_data[warp_id * 32 + ref_pos];\n",
        "            float ref_result = 0.0f;\n",
        "\n",
        "            // computing reference transformation\n",
        "            switch (ref_pos % 4) {\n",
        "                case 0: ref_result = fast_sin(ref_val); break;\n",
        "                case 1: ref_result = fast_cos(ref_val); break;\n",
        "                case 2: ref_result = (ref_val > 0.0f) ? fast_log(ref_val) : -INFINITY; break;\n",
        "                case 3: ref_result = fast_exp(ref_val); break;\n",
        "            }\n",
        "\n",
        "            // computing current transformation and multiplying with reference\n",
        "            switch (mod_4) {\n",
        "                case 0: result = ref_result * fast_sin(current_val); break;\n",
        "                case 1: result = ref_result * fast_cos(current_val); break;\n",
        "                case 2: result = (current_val > 0.0f) ? (ref_result * fast_log(current_val)) : -INFINITY; break;\n",
        "                case 3: result = ref_result * fast_exp(current_val); break;\n",
        "            }\n",
        "        }\n",
        "        shared_data[local_idx + block_size * 4] = result;\n",
        "\n",
        "        // gloabl sum calculation of sin terms according to the problem\n",
        "        if (mod_4 == 0 && lane < 31) {\n",
        "            const int cos_idx = local_idx + 1;\n",
        "            float cos_val = shared_data[cos_idx + block_size * 4];\n",
        "            if (cos_val > 0.5f) {\n",
        "                warp_sin_sum += result;\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "    __syncthreads();\n",
        "\n",
        "    // back to global memory\n",
        "    for (int i = lane; i < 32; i += WARP_SIZE) {\n",
        "        const int global_idx = warp_start + i;\n",
        "        if (global_idx < n) {\n",
        "            const int vec_idx = global_idx / 4;\n",
        "            const int elem_idx = global_idx % 4;\n",
        "            float4* output_vec = &output[vec_idx];\n",
        "            float result_val = shared_data[warp_id * 32 + i + block_size * 4];\n",
        "            switch (elem_idx) {\n",
        "                case 0: output_vec->x = result_val; break;\n",
        "                case 1: output_vec->y = result_val; break;\n",
        "                case 2: output_vec->z = result_val; break;\n",
        "                case 3: output_vec->w = result_val; break;\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "    for (int offset = 16; offset > 0; offset >>= 1) {\n",
        "        warp_sin_sum += __shfl_down_sync(0xFFFFFFFF, warp_sin_sum, offset);\n",
        "    }\n",
        "    if (lane == 0 && warp_sin_sum != 0.0f) {\n",
        "        atomicAdd(global_sum, warp_sin_sum);\n",
        "    }\n",
        "}\n",
        "\n",
        "void measure_performance(int n, int num_runs = 4) {\n",
        "    float4 *d_input, *d_output;\n",
        "    float *d_global_sum, *d_temp;\n",
        "    int n_float4 = (n + 3) / 4;\n",
        "    size_t data_size_bytes = n * sizeof(float);\n",
        "    double data_transferred_bytes = 2.0 * n * sizeof(float); // Read + Write\n",
        "    std::cout << \"performance measurement\\n\";\n",
        "    std::cout << \"array size: \" << n << \"(\"\n",
        "              << std::fixed << std::setprecision(2)\n",
        "              << (data_size_bytes / (1024.0 * 1024.0)) << \" MB)\\n\";\n",
        "\n",
        "    // memory allocation\n",
        "    cudaMalloc(&d_input, n_float4 * sizeof(float4));\n",
        "    cudaMalloc(&d_output, n_float4 * sizeof(float4));\n",
        "    cudaMalloc(&d_global_sum, sizeof(float));\n",
        "    cudaMalloc(&d_temp, data_size_bytes);\n",
        "\n",
        "    // input data generation\n",
        "    std::vector<float4> h_input_float4(n_float4);\n",
        "    for (int i = 0; i < n_float4; i++) {\n",
        "        float4 element;\n",
        "        element.x = 0.1f + static_cast<float>(rand()) / (static_cast<float>(RAND_MAX/4.9f));\n",
        "        element.y = 0.1f + static_cast<float>(rand()) / (static_cast<float>(RAND_MAX/4.9f));\n",
        "        element.z = 0.1f + static_cast<float>(rand()) / (static_cast<float>(RAND_MAX/4.9f));\n",
        "        element.w = 0.1f + static_cast<float>(rand()) / (static_cast<float>(RAND_MAX/4.9f));\n",
        "        h_input_float4[i] = element;\n",
        "    }\n",
        "    cudaMemcpy(d_input, h_input_float4.data(), n_float4 * sizeof(float4), cudaMemcpyHostToDevice);\n",
        "\n",
        "    // using settings from previous iterations (block_size)\n",
        "    const int block_size = 256;\n",
        "    const int grid_size = (n_float4 + block_size - 1) / block_size;\n",
        "    const int shared_mem_size = 2 * block_size * 4 * sizeof(float);\n",
        "\n",
        "    std::cout << \"block size: \" << block_size << \" threads\\n\";\n",
        "    std::cout << \"shared memory: \" << (shared_mem_size / 1024.0) << \" KB per block\\n\";\n",
        "\n",
        "    cudaEvent_t start, stop;\n",
        "    cudaEventCreate(&start);\n",
        "    cudaEventCreate(&stop);\n",
        "\n",
        "    float total_time = 0.0f;\n",
        "    float global_sum_result = 0.0f;\n",
        "\n",
        "    std::cout << \"\\nrunning kernel \" << num_runs << \" time\\n\";\n",
        "\n",
        "    for (int run = 0; run < num_runs; run++) {\n",
        "        float h_global_sum = 0.0f;\n",
        "        cudaMemcpy(d_global_sum, &h_global_sum, sizeof(float), cudaMemcpyHostToDevice);\n",
        "\n",
        "        cudaEventRecord(start);\n",
        "        transform_kernel<<<grid_size, block_size, shared_mem_size>>>(d_input, d_output, d_global_sum, n);\n",
        "        cudaEventRecord(stop);\n",
        "        cudaEventSynchronize(stop);\n",
        "\n",
        "        cudaError_t err = cudaGetLastError();\n",
        "        if (err != cudaSuccess) {\n",
        "            std::cout << \"kernel error: \" << cudaGetErrorString(err) << std::endl;\n",
        "            continue;\n",
        "        }\n",
        "\n",
        "        float kernel_time_ms = 0.0f;\n",
        "        cudaEventElapsedTime(&kernel_time_ms, start, stop);\n",
        "        total_time += kernel_time_ms;\n",
        "\n",
        "        if (run == 0) {\n",
        "            cudaMemcpy(&h_global_sum, d_global_sum, sizeof(float), cudaMemcpyDeviceToHost);\n",
        "            global_sum_result = h_global_sum;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    float avg_time = total_time / num_runs;\n",
        "    double kernel_bandwidth = (data_transferred_bytes / (avg_time / 1000.0)) / (1024.0 * 1024.0 * 1024.0);\n",
        "\n",
        "    cudaEventRecord(start);\n",
        "    cudaMemcpy(d_temp, d_input, data_size_bytes, cudaMemcpyDeviceToDevice);\n",
        "    cudaEventRecord(stop);\n",
        "    cudaEventSynchronize(stop);\n",
        "\n",
        "    float memcpy_time_ms = 0.0f;\n",
        "    cudaEventElapsedTime(&memcpy_time_ms, start, stop);\n",
        "    double memcpy_bandwidth = (data_transferred_bytes / (memcpy_time_ms / 1000.0)) / (1024.0 * 1024.0 * 1024.0);\n",
        "    double efficiency = (kernel_bandwidth / memcpy_bandwidth) * 100.0;\n",
        "\n",
        "    // results\n",
        "    std::cout << \"\\nperformance results:\\n\";\n",
        "    std::cout << \"kernel execution time: \" << std::fixed << std::setprecision(3) << avg_time << \" ms\\n\";\n",
        "    std::cout << \"kernel bandwidth: \" << std::fixed << std::setprecision(2) << kernel_bandwidth << \" GB/s\\n\";\n",
        "    std::cout << \"memory bandwidth: \" << std::fixed << std::setprecision(2) << memcpy_bandwidth << \" GB/s\\n\";\n",
        "    std::cout << \"efficiency: \" << std::fixed << std::setprecision(1) << efficiency << \"%\\n\";\n",
        "    std::cout << \"global sin sum: \" << std::scientific << std::setprecision(6) << global_sum_result << std::endl;\n",
        "\n",
        "    cudaFree(d_input);\n",
        "    cudaFree(d_output);\n",
        "    cudaFree(d_global_sum);\n",
        "    cudaFree(d_temp);\n",
        "    cudaEventDestroy(start);\n",
        "    cudaEventDestroy(stop);\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    const int n = 100000000;\n",
        "    const int num_runs = 4;\n",
        "    srand(42);\n",
        "\n",
        "    cudaDeviceProp prop;\n",
        "    cudaGetDeviceProperties(&prop, 0);\n",
        "    std::cout << \"using google coab for gpu: \" << prop.name << \"\\n\";\n",
        "    std::cout << \"global memory: \" << (prop.totalGlobalMem / (1024.0 * 1024.0 * 1024.0)) << \" GB\\n\";\n",
        "    std::cout << \"shared memory per block: \" << (prop.sharedMemPerBlock / 1024.0) << \" KB\\n\";\n",
        "    std::cout << std::endl;\n",
        "\n",
        "    measure_performance(n, num_runs);\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UK7oEebP7KKe",
        "outputId": "08f774ed-bbab-4905-fbc2-b976462e85db"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing cuda_kernel.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -o cuda_kernel cuda_kernel.cu -std=c++17 -arch=sm_80 -O3 --use_fast_math -Xptxas -O3"
      ],
      "metadata": {
        "id": "l0fP3eXR-Fke"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./cuda_kernel"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F2zYTNQD-Sbr",
        "outputId": "c0bd897b-e2e0-498f-9e7e-87f20f0599ab"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using google coab for gpu: NVIDIA A100-SXM4-40GB\n",
            "global memory: 39.5574 GB\n",
            "shared memory per block: 48 KB\n",
            "\n",
            "performance measurement\n",
            "array size: 100000000(381.47 MB)\n",
            " block size: 256 threads\n",
            " shared memory: 8.00 KB per block\n",
            "\n",
            "running kernel 4 time\n",
            "\n",
            "performance results:\n",
            "kernel execution time: 1.958 ms\n",
            "kernel bandwidth: 380.54 GB/s\n",
            "memory bandwidth: 492.62 GB/s\n",
            "efficiency: 77.2%\n",
            "global sin sum: 1.012631e+05\n"
          ]
        }
      ]
    }
  ]
}
