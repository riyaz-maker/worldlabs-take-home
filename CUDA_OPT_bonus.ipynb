{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ZxB8bwy5jO_",
        "outputId": "5676875f-5f84-44f4-ee39-947cfa592d16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvcc4jupyter\n",
            "  Downloading nvcc4jupyter-1.2.1-py3-none-any.whl.metadata (5.1 kB)\n",
            "Downloading nvcc4jupyter-1.2.1-py3-none-any.whl (10 kB)\n",
            "Installing collected packages: nvcc4jupyter\n",
            "Successfully installed nvcc4jupyter-1.2.1\n"
          ]
        }
      ],
      "source": [
        "!pip install nvcc4jupyter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T19FCLnS7jQC",
        "outputId": "507e574a-d109-4ae6-ef5b-7273b54bc810"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected platform \"Colab\". Running its setup...\n",
            "Source files will be saved in \"/tmp/tmp42ijolwk\".\n"
          ]
        }
      ],
      "source": [
        "%load_ext nvcc4jupyter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lCasiXX77lTA",
        "outputId": "08daf98e-c625-4bd4-ef3f-738053ba20c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing cuda_kernel_bf16.cu\n"
          ]
        }
      ],
      "source": [
        "%%writefile cuda_kernel_bf16.cu\n",
        "#include <cuda_runtime.h>\n",
        "#include <cuda_bf16.h>\n",
        "#include <cmath>\n",
        "#include <iostream>\n",
        "#include <vector>\n",
        "#include <iomanip>\n",
        "#include <cstdlib>\n",
        "\n",
        "#define WARP_SIZE 32\n",
        "\n",
        "// math functions according to the problem statement\n",
        "__device__ __forceinline__ float fast_sin(float x) { return __sinf(x); }\n",
        "__device__ __forceinline__ float fast_cos(float x) { return __cosf(x); }\n",
        "__device__ __forceinline__ float fast_log(float x) { return __logf(x); }\n",
        "__device__ __forceinline__ float fast_exp(float x) { return __expf(x); }\n",
        "\n",
        "// using __nv_bloat16 vectors\n",
        "struct __align__(8) bf16x4 {\n",
        "    __nv_bfloat16 x, y, z, w;\n",
        "};\n",
        "\n",
        "__device__ __forceinline__ float bf2f(__nv_bfloat16 v) { return __bfloat162float(v); }\n",
        "__device__ __forceinline__ __nv_bfloat16 f2bf(float v) { return __float2bfloat16(v); }\n",
        "\n",
        "__global__ void transform_kernel_bf16(const bf16x4* __restrict__ input, bf16x4* __restrict__ output, float* __restrict__ global_sum,int n) {\n",
        "    // shared memory\n",
        "    extern __shared__ float shared_data[];\n",
        "\n",
        "    const int tid = threadIdx.x;\n",
        "    const int lane = tid % WARP_SIZE;\n",
        "    const int warp_id = tid / WARP_SIZE;\n",
        "    const int block_size = blockDim.x;\n",
        "    const int block_start = blockIdx.x * block_size * 4;\n",
        "    const int warp_start  = block_start + warp_id * 32;\n",
        "\n",
        "    // staging input to shared memory\n",
        "    for (int i = lane; i < 32; i += WARP_SIZE) {\n",
        "        const int global_idx = warp_start + i;\n",
        "        if (global_idx < n) {\n",
        "            const int vec_idx  = global_idx / 4;\n",
        "            const int elem_idx = global_idx % 4;\n",
        "            bf16x4 data = input[vec_idx];\n",
        "            float fval = 0.f;\n",
        "            switch (elem_idx) {\n",
        "                case 0: fval = bf2f(data.x); break;\n",
        "                case 1: fval = bf2f(data.y); break;\n",
        "                case 2: fval = bf2f(data.z); break;\n",
        "                case 3: fval = bf2f(data.w); break;\n",
        "            }\n",
        "            shared_data[warp_id * 32 + i] = fval;\n",
        "        }\n",
        "    }\n",
        "    __syncthreads();\n",
        "\n",
        "    float warp_sin_sum = 0.0f;\n",
        "    const int local_idx  = warp_id * 32 + lane;\n",
        "    const int global_idx = warp_start + lane;\n",
        "\n",
        "    if (global_idx < n) {\n",
        "        const int pos_in_32 = lane;\n",
        "        const int mod_4 = pos_in_32 % 4;\n",
        "        const float current_val = shared_data[local_idx];\n",
        "        float result = 0.0f;\n",
        "\n",
        "        // applying transformations\n",
        "        if (pos_in_32 < 16) {\n",
        "            switch (mod_4) {\n",
        "                case 0: result = fast_sin(current_val); break;\n",
        "                case 1: result = fast_cos(current_val); break;\n",
        "                case 2: result = (current_val > 0.0f) ? fast_log(current_val) : -INFINITY; break;\n",
        "                case 3: result = fast_exp(current_val); break;\n",
        "            }\n",
        "        } else {\n",
        "            const int ref_pos = pos_in_32 - 16;\n",
        "            const float ref_val = shared_data[warp_id * 32 + ref_pos];\n",
        "            float ref_result = 0.0f;\n",
        "\n",
        "            switch (ref_pos % 4) {\n",
        "                case 0: ref_result = fast_sin(ref_val); break;\n",
        "                case 1: ref_result = fast_cos(ref_val); break;\n",
        "                case 2: ref_result = (ref_val > 0.0f) ? fast_log(ref_val) : -INFINITY; break;\n",
        "                case 3: ref_result = fast_exp(ref_val); break;\n",
        "            }\n",
        "\n",
        "            switch (mod_4) {\n",
        "                case 0: result = ref_result * fast_sin(current_val); break;\n",
        "                case 1: result = ref_result * fast_cos(current_val); break;\n",
        "                case 2: result = (current_val > 0.0f) ? (ref_result * fast_log(current_val)) : -INFINITY; break;\n",
        "                case 3: result = ref_result * fast_exp(current_val); break;\n",
        "            }\n",
        "        }\n",
        "        shared_data[local_idx + block_size * 4] = result;\n",
        "\n",
        "        // global sum calculation of sin terms according to the problem\n",
        "        if (mod_4 == 0 && lane < 31) {\n",
        "            const int cos_idx = local_idx + 1;\n",
        "            float cos_val = shared_data[cos_idx + block_size * 4];\n",
        "            if (cos_val > 0.5f) {\n",
        "                warp_sin_sum += result;\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "    __syncthreads();\n",
        "\n",
        "    // back to bf16\n",
        "    for (int i = lane; i < 32; i += WARP_SIZE) {\n",
        "        const int gidx = warp_start + i;\n",
        "        if (gidx < n) {\n",
        "            const int vec_idx  = gidx / 4;\n",
        "            const int elem_idx = gidx % 4;\n",
        "            bf16x4* out_vec = &output[vec_idx];\n",
        "            float result_val = shared_data[warp_id * 32 + i + block_size * 4];\n",
        "            __nv_bfloat16 b = f2bf(result_val);\n",
        "            switch (elem_idx) {\n",
        "                case 0: out_vec->x = b; break;\n",
        "                case 1: out_vec->y = b; break;\n",
        "                case 2: out_vec->z = b; break;\n",
        "                case 3: out_vec->w = b; break;\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "    for (int offset = 16; offset > 0; offset >>= 1) {\n",
        "        warp_sin_sum += __shfl_down_sync(0xFFFFFFFF, warp_sin_sum, offset);\n",
        "    }\n",
        "    if (lane == 0 && warp_sin_sum != 0.0f) {\n",
        "        atomicAdd(global_sum, warp_sin_sum);\n",
        "    }\n",
        "}\n",
        "\n",
        "void measure_performance_bf16(int n, int num_runs = 4) {\n",
        "    bf16x4 *d_input, *d_output, *d_temp;\n",
        "    float *d_global_sum;\n",
        "    const int n_vec = (n + 3) / 4;\n",
        "    const size_t vec_bytes = sizeof(bf16x4);\n",
        "    const size_t data_bytes = n_vec * vec_bytes;\n",
        "    const double logical_bytes = 2.0 * static_cast<double>(n) * sizeof(__nv_bfloat16);\n",
        "\n",
        "    std::vector<bf16x4> h_input(n_vec);\n",
        "    for (int i = 0; i < n_vec; i++) {\n",
        "        float x = 0.1f + static_cast<float>(rand()) / (static_cast<float>(RAND_MAX/4.9f));\n",
        "        float y = 0.1f + static_cast<float>(rand()) / (static_cast<float>(RAND_MAX/4.9f));\n",
        "        float z = 0.1f + static_cast<float>(rand()) / (static_cast<float>(RAND_MAX/4.9f));\n",
        "        float w = 0.1f + static_cast<float>(rand()) / (static_cast<float>(RAND_MAX/4.9f));\n",
        "        h_input[i].x = __float2bfloat16(x);\n",
        "        h_input[i].y = __float2bfloat16(y);\n",
        "        h_input[i].z = __float2bfloat16(z);\n",
        "        h_input[i].w = __float2bfloat16(w);\n",
        "    }\n",
        "\n",
        "    cudaMalloc(&d_input,  data_bytes);\n",
        "    cudaMalloc(&d_output, data_bytes);\n",
        "    cudaMalloc(&d_temp,   data_bytes);\n",
        "    cudaMalloc(&d_global_sum, sizeof(float));\n",
        "\n",
        "    cudaMemcpy(d_input, h_input.data(), data_bytes, cudaMemcpyHostToDevice);\n",
        "\n",
        "    const int block_size = 512;\n",
        "    const int grid_size  = (n_vec + block_size - 1) / block_size;\n",
        "    const int shared_mem_size = 2 * block_size * 4 * sizeof(float);\n",
        "\n",
        "    std::cout << \"performance measurement (bf16)\\n\";\n",
        "    std::cout << \"array size: \" << n << \" (\" << std::fixed << std::setprecision(2)\n",
        "              << (static_cast<double>(n) * sizeof(__nv_bfloat16) / (1024.0 * 1024.0)) << \" MB BF16)\\n\";\n",
        "    std::cout << \"block size: \" << block_size << \" threads\\n\";\n",
        "    std::cout << \"shared memory: \" << (shared_mem_size / 1024.0) << \" KB per block\\n\";\n",
        "\n",
        "    cudaEvent_t start, stop; cudaEventCreate(&start); cudaEventCreate(&stop);\n",
        "\n",
        "    float total_time_ms = 0.0f; float global_sum_result = 0.0f;\n",
        "\n",
        "    std::cout << \"\\nrunning kernel \" << num_runs << \" time\\n\";\n",
        "    for (int run = 0; run < num_runs; ++run) {\n",
        "        float zero = 0.0f; cudaMemcpy(d_global_sum, &zero, sizeof(float), cudaMemcpyHostToDevice);\n",
        "\n",
        "        cudaEventRecord(start);\n",
        "        transform_kernel_bf16<<<grid_size, block_size, shared_mem_size>>>(d_input, d_output, d_global_sum, n);\n",
        "        cudaEventRecord(stop);\n",
        "        cudaEventSynchronize(stop);\n",
        "\n",
        "        cudaError_t err = cudaGetLastError();\n",
        "        if (err != cudaSuccess) { std::cout << \"kernel error: \" << cudaGetErrorString(err) << \"\\n\"; continue; }\n",
        "\n",
        "        float ms = 0.0f; cudaEventElapsedTime(&ms, start, stop); total_time_ms += ms;\n",
        "        if (run == 0) { float tmp=0.f; cudaMemcpy(&tmp, d_global_sum, sizeof(float), cudaMemcpyDeviceToHost); global_sum_result = tmp; }\n",
        "    }\n",
        "\n",
        "    const float avg_ms = total_time_ms / num_runs;\n",
        "    const double kernel_bw_GBps = (logical_bytes / (avg_ms / 1000.0)) / (1024.0*1024.0*1024.0);\n",
        "\n",
        "    cudaEventRecord(start);\n",
        "    cudaMemcpy(d_temp, d_input, data_bytes, cudaMemcpyDeviceToDevice);\n",
        "    cudaEventRecord(stop);\n",
        "    cudaEventSynchronize(stop);\n",
        "\n",
        "    float memcpy_ms = 0.0f; cudaEventElapsedTime(&memcpy_ms, start, stop);\n",
        "    const double memcpy_bw_GBps = (logical_bytes / (memcpy_ms / 1000.0)) / (1024.0*1024.0*1024.0);\n",
        "    const double efficiency = (kernel_bw_GBps / memcpy_bw_GBps) * 100.0;\n",
        "\n",
        "    std::cout << \"\\nperformance results (bf16):\\n\";\n",
        "    std::cout << \"kernel execution time: \" << std::fixed << std::setprecision(3) << avg_ms << \" ms\\n\";\n",
        "    std::cout << \"kernel bandwidth: \" << std::fixed << std::setprecision(2) << kernel_bw_GBps << \" GB/s\\n\";\n",
        "    std::cout << \"memory bandwidth: \" << std::fixed << std::setprecision(2) << memcpy_bw_GBps << \" GB/s\\n\";\n",
        "    std::cout << \"efficiency: \" << std::fixed << std::setprecision(1) << efficiency << \"%\\n\";\n",
        "    std::cout << \"global sin sum: \" << std::scientific << std::setprecision(6) << global_sum_result << \"\\n\";\n",
        "\n",
        "    cudaEventDestroy(start); cudaEventDestroy(stop);\n",
        "    cudaFree(d_input); cudaFree(d_output); cudaFree(d_temp); cudaFree(d_global_sum);\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    const int n = 100000000;\n",
        "    const int num_runs = 4;\n",
        "    srand(42);\n",
        "\n",
        "    cudaDeviceProp prop; cudaGetDeviceProperties(&prop, 0);\n",
        "    std::cout << \"using google colab for gpu: \" << prop.name << \"\\n\";\n",
        "    std::cout << \"global memory: \" << (prop.totalGlobalMem / (1024.0 * 1024.0 * 1024.0)) << \" GB\\n\";\n",
        "    std::cout << \"shared memory per block: \" << (prop.sharedMemPerBlock / 1024.0) << \" KB\\n\\n\";\n",
        "\n",
        "    measure_performance_bf16(n, num_runs);\n",
        "    return 0;\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Y4w9k0cD7oaP"
      },
      "outputs": [],
      "source": [
        "!nvcc -o cuda_kernel_bf16 cuda_kernel_bf16.cu -std=c++17 -arch=sm_80 -O3 -Xptxas -O3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pr7vDbjR7xvp",
        "outputId": "1c8f61d8-2f0d-4b85-a63f-a050588b2d8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using google colab for gpu: NVIDIA A100-SXM4-40GB\n",
            "global memory: 39.5574 GB\n",
            "shared memory per block: 48 KB\n",
            "\n",
            "performance measurement (bf16)\n",
            "array size: 100000000 (190.73 MB BF16)\n",
            "block size: 512 threads\n",
            "shared memory: 16.00 KB per block\n",
            "\n",
            "running kernel 4 time\n",
            "\n",
            "performance results (bf16):\n",
            "kernel execution time: 1.899 ms\n",
            "kernel bandwidth: 196.17 GB/s\n",
            "memory bandwidth: 288.04 GB/s\n",
            "efficiency: 68.1%\n",
            "global sin sum: 1.008407e+05\n"
          ]
        }
      ],
      "source": [
        "!./cuda_kernel_bf16"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}